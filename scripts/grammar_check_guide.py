#!/usr/bin/env python3
"""
è®ºæ–‡è¯­æ³•æ£€æŸ¥ä¸å¥å¼ä¼˜åŒ–æŒ‡å—

ç”±äºæ— æ³•ç›´æ¥è°ƒç”¨Grammarly API,æ­¤è„šæœ¬æä¾›:
1. å¸¸è§è¯­æ³•é—®é¢˜è‡ªåŠ¨æ£€æµ‹
2. å­¦æœ¯å†™ä½œé£æ ¼æ£€æŸ¥
3. å†—ä½™è¡¨è¾¾è¯†åˆ«
4. è¢«åŠ¨è¯­æ€æ£€æµ‹
5. å¥å­é•¿åº¦åˆ†æ
"""

import re
from pathlib import Path
from collections import defaultdict

# å¸¸è§è¯­æ³•é—®é¢˜æ¨¡å¼
GRAMMAR_PATTERNS = {
    'a_vs_an': [
        (r'\ba ([aeiou])', r'an \1', 'Use "an" before vowel sounds'),
        (r'\ban ([^aeiou])', r'a \1', 'Use "a" before consonant sounds'),
    ],
    'subject_verb_agreement': [
        (r'\b(data|criteria|phenomena) is\b', r'\1 are', 'Plural subject needs "are"'),
        (r'\b(datum|criterion|phenomenon) are\b', r'\1 is', 'Singular subject needs "is"'),
    ],
    'redundant_phrases': [
        (r'\bvery unique\b', 'unique', '"unique" already means "one of a kind"'),
        (r'\bmore optimal\b', 'optimal', '"optimal" already means "best"'),
        (r'\bmore superior\b', 'superior', '"superior" already means "better"'),
        (r'\bin order to\b', 'to', 'Often "to" alone is sufficient'),
        (r'\bdue to the fact that\b', 'because', 'More concise'),
        (r'\bat this point in time\b', 'now', 'More concise'),
    ],
    'wordy_constructions': [
        (r'\bmake use of\b', 'use', 'Simpler'),
        (r'\bgive consideration to\b', 'consider', 'Simpler'),
        (r'\btake into account\b', 'consider', 'Simpler'),
        (r'\bby means of\b', 'by', 'Simpler'),
        (r'\bfor the purpose of\b', 'for', 'Simpler'),
    ],
}

# å­¦æœ¯å†™ä½œé£æ ¼é—®é¢˜
STYLE_PATTERNS = {
    'contractions': [
        (r"\bdon't\b", "do not", "Avoid contractions in academic writing"),
        (r"\bcan't\b", "cannot", "Avoid contractions"),
        (r"\bwon't\b", "will not", "Avoid contractions"),
        (r"\bwe're\b", "we are", "Avoid contractions"),
        (r"\bit's\b", "it is", "Avoid contractions (or use 'its' for possessive)"),
    ],
    'informal_language': [
        (r'\ba lot of\b', 'many/much', 'Too informal'),
        (r'\bkinda\b', 'somewhat', 'Too informal'),
        (r'\bgonna\b', 'going to', 'Too informal'),
        (r'\bstuff\b', 'things/items', 'Too informal'),
        (r'\bget\b', 'obtain/achieve/receive', 'Often too informal'),
    ],
    'vague_qualifiers': [
        (r'\bvery\b', '[consider removing or be specific]', 'Often unnecessary'),
        (r'\breally\b', '[consider removing]', 'Often adds little meaning'),
        (r'\bquite\b', '[be specific]', 'Vague quantifier'),
        (r'\bsomewhat\b', '[be specific]', 'Vague quantifier'),
    ],
}

# è¢«åŠ¨è¯­æ€æ£€æµ‹ (ç®€åŒ–ç‰ˆ)
PASSIVE_VOICE_PATTERNS = [
    r'\bis (being )?([\w]+ed|shown|made|given|obtained|achieved)\b',
    r'\bare (being )?([\w]+ed|shown|made|given|obtained|achieved)\b',
    r'\bwas (being )?([\w]+ed|shown|made|given|obtained|achieved)\b',
    r'\bwere (being )?([\w]+ed|shown|made|given|obtained|achieved)\b',
    r'\bhas been ([\w]+ed|shown|made|given|obtained|achieved)\b',
    r'\bhave been ([\w]+ed|shown|made|given|obtained|achieved)\b',
]

def check_sentence_length(text):
    """æ£€æŸ¥å¥å­é•¿åº¦"""
    sentences = re.split(r'[.!?]+', text)
    long_sentences = []

    for i, sent in enumerate(sentences):
        words = len(sent.split())
        if words > 40:
            long_sentences.append({
                'sentence': sent.strip()[:100] + '...',
                'words': words,
                'recommendation': 'Consider breaking into multiple sentences'
            })

    return long_sentences

def check_passive_voice(text):
    """æ£€æµ‹è¢«åŠ¨è¯­æ€"""
    passive_instances = []

    for pattern in PASSIVE_VOICE_PATTERNS:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        for match in matches:
            context_start = max(0, match.start() - 50)
            context_end = min(len(text), match.end() + 50)
            context = text[context_start:context_end]

            passive_instances.append({
                'phrase': match.group(),
                'context': context,
                'recommendation': 'Consider active voice if possible'
            })

    return passive_instances

def check_grammar_and_style(text, filename):
    """æ£€æŸ¥è¯­æ³•å’Œé£æ ¼"""
    issues = defaultdict(list)

    # è¯­æ³•æ£€æŸ¥
    for category, patterns in GRAMMAR_PATTERNS.items():
        for pattern, replacement, explanation in patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                issues[category].append({
                    'original': match.group(),
                    'suggested': replacement,
                    'explanation': explanation,
                    'position': match.start()
                })

    # é£æ ¼æ£€æŸ¥
    for category, patterns in STYLE_PATTERNS.items():
        for pattern, replacement, explanation in patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                issues[category].append({
                    'original': match.group(),
                    'suggested': replacement,
                    'explanation': explanation,
                    'position': match.start()
                })

    return issues

def analyze_file(filepath):
    """åˆ†æå•ä¸ªæ–‡ä»¶"""
    print(f"\n{'='*80}")
    print(f"åˆ†ææ–‡ä»¶: {filepath.name}")
    print('='*80)

    with open(filepath, 'r', encoding='utf-8') as f:
        content = f.read()

    # åªåˆ†æè‹±æ–‡å†…å®¹ (æ’é™¤ä»£ç å—å’Œä¸­æ–‡éƒ¨åˆ†)
    # ç®€åŒ–: æå–## æ ‡é¢˜ä¹‹åçš„æ®µè½
    english_sections = []
    lines = content.split('\n')
    in_english_section = False

    for line in lines:
        if line.startswith('## ') and not any(ord(c) > 127 for c in line):
            in_english_section = True
        elif line.startswith('## ') and any(ord(c) > 127 for c in line):
            in_english_section = False
        elif in_english_section and line.strip() and not line.startswith('```'):
            english_sections.append(line)

    english_text = ' '.join(english_sections)

    # è¯­æ³•å’Œé£æ ¼æ£€æŸ¥
    issues = check_grammar_and_style(english_text, filepath.name)

    total_issues = sum(len(v) for v in issues.values())

    if total_issues == 0:
        print("  âœ… æœªå‘ç°å¸¸è§è¯­æ³•/é£æ ¼é—®é¢˜")
    else:
        print(f"\nå‘ç° {total_issues} ä¸ªæ½œåœ¨é—®é¢˜:\n")

        for category, problem_list in issues.items():
            if problem_list:
                print(f"\nã€{category.upper()}ã€‘({len(problem_list)}å¤„)")
                for i, problem in enumerate(problem_list[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ª
                    print(f"  {i}. '{problem['original']}' â†’ '{problem['suggested']}'")
                    print(f"     è¯´æ˜: {problem['explanation']}")
                if len(problem_list) > 3:
                    print(f"  ... è¿˜æœ‰ {len(problem_list) - 3} å¤„ç±»ä¼¼é—®é¢˜")

    # å¥å­é•¿åº¦æ£€æŸ¥
    long_sentences = check_sentence_length(english_text)
    if long_sentences:
        print(f"\nã€LONG SENTENCESã€‘({len(long_sentences)}å¤„)")
        for i, sent_info in enumerate(long_sentences[:3], 1):
            print(f"  {i}. {sent_info['words']} words: {sent_info['sentence']}")
            print(f"     {sent_info['recommendation']}")
        if len(long_sentences) > 3:
            print(f"  ... è¿˜æœ‰ {len(long_sentences) - 3} ä¸ªé•¿å¥")

    # è¢«åŠ¨è¯­æ€æ£€æŸ¥ (æŠ½æ ·)
    passive_instances = check_passive_voice(english_text)
    if len(passive_instances) > 10:
        print(f"\nã€PASSIVE VOICEã€‘(å‘ç° {len(passive_instances)}å¤„)")
        print(f"  æç¤º: å­¦æœ¯å†™ä½œä¸­è¢«åŠ¨è¯­æ€å¯æ¥å—,ä½†ä¸»åŠ¨è¯­æ€é€šå¸¸æ›´æ¸…æ™°")
        print(f"  å»ºè®®: æ£€æŸ¥æ˜¯å¦å¯ä»¥æ”¹ä¸ºä¸»åŠ¨è¯­æ€ä»¥å¢å¼ºå¯è¯»æ€§")

    return {
        'grammar_style_issues': total_issues,
        'long_sentences': len(long_sentences),
        'passive_voice': len(passive_instances)
    }

def generate_optimization_suggestions():
    """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
    print(f"\n{'='*80}")
    print("è¯­æ³•ä¼˜åŒ–é€šç”¨å»ºè®®")
    print('='*80)

    suggestions = """
ã€å­¦æœ¯å†™ä½œæœ€ä½³å®è·µã€‘

1. **æ¸…æ™°æ€§ä¼˜å…ˆ**
   - ä¼˜å…ˆä½¿ç”¨ä¸»åŠ¨è¯­æ€ (We propose... vs. It is proposed...)
   - é¿å…å†—é•¿å¥å­ (ç†æƒ³é•¿åº¦: 15-25è¯)
   - ä½¿ç”¨å…·ä½“æœ¯è¯­è€Œéæ¨¡ç³Šé™å®šè¯

2. **ä¸€è‡´æ€§**
   - æ—¶æ€ä¸€è‡´ (Introduction: present tense, Method: past tense)
   - æœ¯è¯­ä¸€è‡´ (å·²é€šè¿‡æ£€æŸ¥ âœ…)
   - å¼•ç”¨æ ¼å¼ä¸€è‡´

3. **ç®€æ´æ€§**
   - åˆ é™¤å†—ä½™è¯æ±‡ ("in order to" â†’ "to")
   - é¿å…åè¯åŒ– ("conduct an analysis" â†’ "analyze")
   - åˆ é™¤æ— æ„ä¹‰çš„é™å®šè¯ ("very", "really")

4. **ä¸“ä¸šæ€§**
   - é¿å…å£è¯­åŒ–è¡¨è¾¾ ("a lot of" â†’ "many")
   - é¿å…ç¼©å†™ ("don't" â†’ "do not")
   - ä½¿ç”¨é¢†åŸŸæ ‡å‡†æœ¯è¯­

5. **é€»è¾‘æµç•…æ€§**
   - ä½¿ç”¨è¿‡æ¸¡è¯è¿æ¥å¥å­ (However, Moreover, Therefore)
   - æ®µè½ä¸»é¢˜å¥æ˜ç¡®
   - æ¯æ®µèšç„¦å•ä¸€è§‚ç‚¹

ã€æ¨èå·¥å…·ã€‘

1. **Grammarly** (ä»˜è´¹ç‰ˆ)
   - å®æ—¶è¯­æ³•æ£€æŸ¥
   - å­¦æœ¯å†™ä½œæ¨¡å¼
   - å¥å¼ä¼˜åŒ–å»ºè®®

2. **QuillBot** (å…è´¹/ä»˜è´¹)
   - Paraphrasing tool
   - Grammar checker
   - å¥å¼å˜åŒ–å»ºè®®

3. **ChatGPT/Claude** (AIè¾…åŠ©)
   - é€æ®µæ¶¦è‰²
   - æä¾›æ”¹å†™å»ºè®®
   - è§£é‡Šè¯­æ³•è§„åˆ™

ã€æ‰‹åŠ¨æ£€æŸ¥æ¸…å•ã€‘

â–¡ æ‰€æœ‰å¥å­ä¸»è°“å®¾æ¸…æ™°
â–¡ æ²¡æœ‰æ‚¬å‚ä¿®é¥°è¯­
â–¡ ä»£è¯æŒ‡ä»£æ˜ç¡® (this/that/it æŒ‡ä»£æ¸…æ¥š)
â–¡ å¹³è¡Œç»“æ„æ­£ç¡® (A, B, and C æ ¼å¼ä¸€è‡´)
â–¡ æ ‡ç‚¹ç¬¦å·æ­£ç¡® (é€—å·ã€åˆ†å·ä½¿ç”¨è§„èŒƒ)
â–¡ å¼•ç”¨æ ¼å¼ç»Ÿä¸€
â–¡ é¦–å­—æ¯ç¼©å†™é¦–æ¬¡ä½¿ç”¨æ—¶å®šä¹‰
â–¡ æ•°å­—æ ¼å¼ä¸€è‡´ (ä½•æ—¶ç”¨é˜¿æ‹‰ä¼¯æ•°å­—,ä½•æ—¶æ‹¼å†™)
"""

    print(suggestions)

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*80)
    print("FedForget è®ºæ–‡è¯­æ³•ä¸é£æ ¼æ£€æŸ¥")
    print("="*80)

    # å¾…æ£€æŸ¥çš„æ–‡ä»¶
    files_to_check = [
        Path("PAPER_INTRODUCTION_RELATEDWORK.md"),
        Path("PAPER_METHOD_SECTION.md"),
        Path("PAPER_EXPERIMENTS_SECTION.md"),
        Path("PAPER_ABSTRACT_DISCUSSION_CONCLUSION.md"),
    ]

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    missing = [f for f in files_to_check if not f.exists()]
    if missing:
        print(f"\nâš ï¸  ç¼ºå¤±æ–‡ä»¶: {missing}")
        return

    results = {}

    # åˆ†ææ¯ä¸ªæ–‡ä»¶
    for filepath in files_to_check:
        results[filepath.name] = analyze_file(filepath)

    # æ€»ç»“
    print(f"\n{'='*80}")
    print("æ£€æŸ¥æ€»ç»“")
    print('='*80)

    total_grammar = sum(r['grammar_style_issues'] for r in results.values())
    total_long = sum(r['long_sentences'] for r in results.values())
    total_passive = sum(r['passive_voice'] for r in results.values())

    print(f"\nå‘ç°é—®é¢˜ç»Ÿè®¡:")
    print(f"  è¯­æ³•/é£æ ¼é—®é¢˜: {total_grammar}å¤„")
    print(f"  é•¿å¥ (>40è¯): {total_long}å¤„")
    print(f"  è¢«åŠ¨è¯­æ€: {total_passive}å¤„")

    print(f"\nè¯¦ç»†ç»“æœ:")
    for filename, stats in results.items():
        print(f"  {filename}:")
        print(f"    - è¯­æ³•/é£æ ¼: {stats['grammar_style_issues']}")
        print(f"    - é•¿å¥: {stats['long_sentences']}")
        print(f"    - è¢«åŠ¨è¯­æ€: {stats['passive_voice']}")

    # ç”Ÿæˆå»ºè®®
    generate_optimization_suggestions()

    print("\n" + "="*80)
    print("ä¸‹ä¸€æ­¥è¡ŒåŠ¨:")
    print("="*80)
    print("""
1. âœ… å·²å®ŒæˆåŸºç¡€è¯­æ³•æ£€æŸ¥ (è‡ªåŠ¨åŒ–)

2. ğŸ“ æ¨èæ‰‹åŠ¨æ“ä½œ:
   a) ä½¿ç”¨Grammarly/QuillBotæ£€æŸ¥4ä¸ªç« èŠ‚
   b) é€æ®µå¤åˆ¶åˆ°AIå·¥å…· (ChatGPT/Claude) è¿›è¡Œæ¶¦è‰²
   c) é‡ç‚¹å…³æ³¨:
      - Introduction (å¸å¼•è¯»è€…)
      - Abstract (ç®€æ´æœ‰åŠ›)
      - Conclusion (å¼ºè°ƒè´¡çŒ®)

3. â­ï¸  å®Œæˆè¯­æ³•æ£€æŸ¥å:
   - è¿è¡Œä¸€è‡´æ€§æ£€æŸ¥ç¡®è®¤æœªå¼•å…¥æ–°é—®é¢˜
   - è¿›å…¥LaTeXè½¬æ¢é˜¶æ®µ

é¢„è®¡æ—¶é—´: 2-3å°æ—¶äººå·¥æ¶¦è‰²
    """)
    print("="*80 + "\n")

if __name__ == "__main__":
    main()
