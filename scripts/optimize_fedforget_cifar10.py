#!/usr/bin/env python3
"""
CIFAR-10ä¸Šçš„FedForgetå‚æ•°ä¼˜åŒ–

ç›®æ ‡:
1. æ‰¾åˆ°æœ€ä½³çš„alphaå’Œlambda_negç»„åˆ
2. æå‡é—å¿˜ç‡åˆ°>30%
3. ä¿æŒæµ‹è¯•å‡†ç¡®ç‡>90%

æœç´¢ç©ºé—´:
- alpha: [0.85, 0.88, 0.90, 0.92, 0.95, 0.97]
- lambda_neg: [2.0, 3.0, 5.0, 8.0]
- lambda_forget: [1.5] (å›ºå®š)

ç­–ç•¥:
- æ—©åœ: å¦‚æœtest_acc < 50%, è·³è¿‡ï¼ˆæ¨¡å‹å´©æºƒï¼‰
- è®°å½•æ‰€æœ‰ç»“æœåˆ°CSV
"""

import sys
sys.path.append('/home/featurize/work/GJC/fedforget')

import torch
import numpy as np
import time
import csv
from itertools import product

from src.data import load_federated_data
from src.models import ConvNet
from src.federated import Client, Server
from src.federated.server import FedForgetServer
from src.federated.client import UnlearningClient
from src.utils.metrics import evaluate_model

# å›ºå®šéšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)
device = 'cuda' if torch.cuda.is_available() else 'cpu'

print("="*80)
print("CIFAR-10ä¸Šçš„FedForgetå‚æ•°ä¼˜åŒ–")
print("="*80)

# ============================================================
# æ•°æ®å‡†å¤‡ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰
# ============================================================
print("\n[1/3] æ•°æ®åŠ è½½...")
fed_data = load_federated_data(
    dataset_name='cifar10',
    num_clients=5,
    data_dist='noniid',
    dirichlet_alpha=0.5,
    data_root='/home/featurize/data'
)

test_loader = fed_data.get_test_loader(batch_size=256)
client0_loader = fed_data.get_client_loader(0, batch_size=64)

print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ")

# ============================================================
# é¢„è®­ç»ƒï¼ˆåªè®­ç»ƒä¸€æ¬¡ï¼Œæ‰€æœ‰å‚æ•°æœç´¢å…±ç”¨ï¼‰
# ============================================================
print("\n[2/3] é¢„è®­ç»ƒæ¨¡å‹ (20è½®)...")
model = ConvNet(num_classes=10, num_channels=3)
server = Server(model=model, device=device)

clients = []
for i in range(5):
    client_loader = fed_data.get_client_loader(i, batch_size=64)
    client = Client(
        client_id=i,
        model=ConvNet(num_classes=10, num_channels=3),
        data_loader=client_loader,
        device=device,
        lr=0.01
    )
    clients.append(client)

pretrain_start = time.time()
for round_idx in range(20):
    global_params = server.get_model_parameters()
    client_models = []
    client_weights = []

    for client in clients:
        client.set_model_parameters(global_params)
        client.local_train(epochs=2, verbose=False)
        client_models.append(client.get_model_parameters())
        client_weights.append(client.num_samples)

    aggregated = server.aggregate(client_models, client_weights)
    server.set_model_parameters(aggregated)

pretrain_time = time.time() - pretrain_start

pretrain_test = server.evaluate(test_loader)
pretrain_forget = evaluate_model(server.model, client0_loader, device=device)

print(f"âœ“ é¢„è®­ç»ƒå®Œæˆ (è€—æ—¶: {pretrain_time:.1f}s)")
print(f"  æµ‹è¯•å‡†ç¡®ç‡: {pretrain_test['accuracy']:.2f}%")
print(f"  é—å¿˜æ•°æ®å‡†ç¡®ç‡: {pretrain_forget['accuracy']:.2f}%")

pretrained_params = server.get_model_parameters()

# ============================================================
# å‚æ•°æœç´¢
# ============================================================
print("\n[3/3] å‚æ•°æœç´¢...")

# æœç´¢ç©ºé—´
alpha_values = [0.85, 0.88, 0.90, 0.92, 0.95, 0.97]
lambda_neg_values = [2.0, 3.0, 5.0, 8.0]
lambda_forget = 1.5  # å›ºå®š

total_configs = len(alpha_values) * len(lambda_neg_values)
print(f"æœç´¢ç©ºé—´: {len(alpha_values)} alphas Ã— {len(lambda_neg_values)} lambda_negs = {total_configs} é…ç½®")
print()

results = []
config_idx = 0

for alpha, lambda_neg in product(alpha_values, lambda_neg_values):
    config_idx += 1
    print(f"[{config_idx}/{total_configs}] æµ‹è¯•: alpha={alpha}, lambda_neg={lambda_neg}")

    # åˆ›å»ºFedForgetServer
    fedforget_server = FedForgetServer(
        model=ConvNet(num_classes=10, num_channels=3),
        device=device
    )
    fedforget_server.set_model_parameters(pretrained_params)
    fedforget_server.lambda_forget = lambda_forget

    # åˆ›å»ºé—å¿˜å®¢æˆ·ç«¯
    unlearn_client = UnlearningClient(
        client_id=0,
        model=ConvNet(num_classes=10, num_channels=3),
        data_loader=client0_loader,
        device=device,
        lr=0.005
    )

    unlearn_client.prepare_unlearning(
        global_model_params=pretrained_params,
        local_model_params=None
    )

    fedforget_server.register_unlearning_client(0, current_round=0)
    regular_clients = [clients[1], clients[2], clients[3], clients[4]]

    # é—å¿˜è®­ç»ƒ
    start_time = time.time()
    early_stop = False

    for round_idx in range(10):
        global_params = fedforget_server.get_model_parameters()

        # é—å¿˜å®¢æˆ·ç«¯è®­ç»ƒ
        unlearn_client.set_model_parameters(global_params)
        unlearn_client.unlearning_train(
            epochs=2,
            method='dual_teacher',
            distill_temp=2.0,
            alpha=alpha,
            lambda_pos=1.0,
            lambda_neg=lambda_neg,
            verbose=False
        )

        # éšæœºé€‰æ‹©2ä¸ªå¸¸è§„å®¢æˆ·ç«¯
        selected_regular = np.random.choice(regular_clients, 2, replace=False)

        client_models = [unlearn_client.get_model_parameters()]
        client_ids = [0]
        client_samples = [unlearn_client.num_samples]

        for client in selected_regular:
            client.set_model_parameters(global_params)
            client.local_train(epochs=2, verbose=False)
            client_models.append(client.get_model_parameters())
            client_ids.append(client.client_id)
            client_samples.append(client.num_samples)

        # èšåˆ
        aggregated = fedforget_server.aggregate_with_fedforget(
            client_models,
            client_ids,
            client_samples,
            current_round=round_idx
        )
        fedforget_server.set_model_parameters(aggregated)

        # æ—©åœæ£€æµ‹ï¼ˆæ¯2è½®æ£€æŸ¥ä¸€æ¬¡ï¼‰
        if (round_idx + 1) % 2 == 0:
            temp_test = fedforget_server.evaluate(test_loader)
            if temp_test['accuracy'] < 50.0:
                print(f"  âš ï¸ æ—©åœ! Round {round_idx+1}, Test={temp_test['accuracy']:.2f}% (æ¨¡å‹å´©æºƒ)")
                early_stop = True
                break

    exec_time = time.time() - start_time

    # æœ€ç»ˆè¯„ä¼°
    if early_stop:
        test_acc = temp_test['accuracy']
        forget_acc = 0.0  # å´©æºƒæƒ…å†µä¸‹è®¾ä¸º0
        status = "CRASHED"
    else:
        test_results = fedforget_server.evaluate(test_loader)
        forget_results = evaluate_model(fedforget_server.model, client0_loader, device=device)
        test_acc = test_results['accuracy']
        forget_acc = forget_results['accuracy']
        status = "OK"

    # è®¡ç®—æŒ‡æ ‡
    retention = test_acc / pretrain_test['accuracy']
    forgetting = (pretrain_forget['accuracy'] - forget_acc) / pretrain_forget['accuracy']

    # è®°å½•ç»“æœ
    result = {
        'alpha': alpha,
        'lambda_neg': lambda_neg,
        'lambda_forget': lambda_forget,
        'test_acc': test_acc,
        'forget_acc': forget_acc,
        'retention': retention * 100,
        'forgetting': forgetting * 100,
        'time': exec_time,
        'status': status
    }
    results.append(result)

    # æ‰“å°ç»“æœ
    if status == "OK":
        print(f"  âœ“ Test: {test_acc:.2f}%, Forget: {forget_acc:.2f}%, "
              f"ä¿æŒç‡: {retention*100:.1f}%, é—å¿˜ç‡: {forgetting*100:.1f}%, "
              f"è€—æ—¶: {exec_time:.1f}s")
    else:
        print(f"  âœ— æ¨¡å‹å´©æºƒ")

    print()

# ============================================================
# ç»“æœæ±‡æ€»
# ============================================================
print("="*80)
print("å‚æ•°æœç´¢å®Œæˆ")
print("="*80)

# ä¿å­˜åˆ°CSV
csv_path = '/home/featurize/work/GJC/fedforget/results/fedforget_optimization_cifar10.csv'
import os
os.makedirs('/home/featurize/work/GJC/fedforget/results', exist_ok=True)

with open(csv_path, 'w', newline='') as f:
    writer = csv.DictWriter(f, fieldnames=['alpha', 'lambda_neg', 'lambda_forget',
                                            'test_acc', 'forget_acc', 'retention',
                                            'forgetting', 'time', 'status'])
    writer.writeheader()
    writer.writerows(results)

print(f"\nâœ“ ç»“æœå·²ä¿å­˜åˆ°: {csv_path}")

# è¿‡æ»¤æœ‰æ•ˆç»“æœ
valid_results = [r for r in results if r['status'] == 'OK']

if len(valid_results) == 0:
    print("\nâš ï¸ æ‰€æœ‰é…ç½®éƒ½å´©æºƒäº†! éœ€è¦è°ƒæ•´æœç´¢ç©ºé—´")
else:
    print(f"\næœ‰æ•ˆé…ç½®: {len(valid_results)}/{len(results)}")

    # æ’åºï¼šæŒ‰é—å¿˜ç‡é™åº
    valid_results_sorted = sorted(valid_results, key=lambda x: x['forgetting'], reverse=True)

    print("\n" + "="*80)
    print("Top 10 é…ç½® (æŒ‰é—å¿˜ç‡æ’åº)")
    print("="*80)
    print(f"{'Rank':<6} {'Alpha':<8} {'Î»_neg':<8} {'Test%':<8} {'Forget%':<9} {'ä¿æŒç‡':<9} {'é—å¿˜ç‡':<9} {'è€—æ—¶':<8}")
    print("-"*80)

    for i, r in enumerate(valid_results_sorted[:10], 1):
        # æ£€æŸ¥æ˜¯å¦æ»¡è¶³ç›®æ ‡
        success = r['retention'] > 90.0 and r['forgetting'] > 30.0
        marker = "ğŸ¯" if success else ""

        print(f"{i:<6} {r['alpha']:<8.2f} {r['lambda_neg']:<8.1f} {r['test_acc']:<8.2f} "
              f"{r['forget_acc']:<9.2f} {r['retention']:<9.1f} {r['forgetting']:<9.1f} "
              f"{r['time']:<8.1f} {marker}")

    print("="*80)

    # æ‰¾åˆ°æœ€ä½³å¹³è¡¡é…ç½®
    print("\næ¨èé…ç½®:")

    # ç­–ç•¥1: æœ€å¤§é—å¿˜ç‡ï¼ˆä¿æŒç‡>85%ï¼‰
    high_forgetting = [r for r in valid_results if r['retention'] > 85.0]
    if high_forgetting:
        best_forgetting = max(high_forgetting, key=lambda x: x['forgetting'])
        print(f"\n1. æœ€å¤§é—å¿˜ç‡é…ç½® (ä¿æŒç‡>85%):")
        print(f"   alpha={best_forgetting['alpha']}, lambda_neg={best_forgetting['lambda_neg']}")
        print(f"   â†’ é—å¿˜ç‡: {best_forgetting['forgetting']:.1f}%, ä¿æŒç‡: {best_forgetting['retention']:.1f}%")

    # ç­–ç•¥2: æœ€ä½³å¹³è¡¡ï¼ˆä¿æŒç‡>90%, é—å¿˜ç‡>20%ï¼‰
    balanced = [r for r in valid_results if r['retention'] > 90.0 and r['forgetting'] > 20.0]
    if balanced:
        best_balanced = max(balanced, key=lambda x: x['forgetting'])
        print(f"\n2. æœ€ä½³å¹³è¡¡é…ç½® (ä¿æŒç‡>90%, é—å¿˜ç‡>20%):")
        print(f"   alpha={best_balanced['alpha']}, lambda_neg={best_balanced['lambda_neg']}")
        print(f"   â†’ é—å¿˜ç‡: {best_balanced['forgetting']:.1f}%, ä¿æŒç‡: {best_balanced['retention']:.1f}%")

    # ç­–ç•¥3: æœ€é«˜ä¿æŒç‡ï¼ˆé—å¿˜ç‡>20%ï¼‰
    high_retention = [r for r in valid_results if r['forgetting'] > 20.0]
    if high_retention:
        best_retention = max(high_retention, key=lambda x: x['retention'])
        print(f"\n3. æœ€é«˜ä¿æŒç‡é…ç½® (é—å¿˜ç‡>20%):")
        print(f"   alpha={best_retention['alpha']}, lambda_neg={best_retention['lambda_neg']}")
        print(f"   â†’ é—å¿˜ç‡: {best_retention['forgetting']:.1f}%, ä¿æŒç‡: {best_retention['retention']:.1f}%")

    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
    goal_met = [r for r in valid_results if r['retention'] > 90.0 and r['forgetting'] > 30.0]
    if goal_met:
        print(f"\nğŸ‰ æˆåŠŸ! æ‰¾åˆ°{len(goal_met)}ä¸ªé…ç½®åŒæ—¶æ»¡è¶³:")
        print(f"   - ä¿æŒç‡ > 90%")
        print(f"   - é—å¿˜ç‡ > 30%")
        best = max(goal_met, key=lambda x: x['forgetting'])
        print(f"\n   æœ€ä½³é…ç½®: alpha={best['alpha']}, lambda_neg={best['lambda_neg']}")
        print(f"   é—å¿˜ç‡: {best['forgetting']:.1f}%, ä¿æŒç‡: {best['retention']:.1f}%")
    else:
        print(f"\nğŸ’¡ æç¤º: æœªæ‰¾åˆ°åŒæ—¶æ»¡è¶³ä¿æŒç‡>90%å’Œé—å¿˜ç‡>30%çš„é…ç½®")
        print(f"   å¯èƒ½éœ€è¦:")
        print(f"   1. è¿›ä¸€æ­¥é™ä½alpha (æµ‹è¯•0.80-0.85)")
        print(f"   2. å¢å¤§lambda_neg (æµ‹è¯•10.0+)")
        print(f"   3. è°ƒæ•´lambda_forget (æµ‹è¯•2.0+)")

print("\nå®éªŒå®Œæˆ!")
