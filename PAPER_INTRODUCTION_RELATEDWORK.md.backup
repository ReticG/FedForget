# è®ºæ–‡ Introduction & Related Work ç« èŠ‚è‰ç¨¿ ğŸ“

**æ’°å†™æ—¶é—´**: 2025-10-06
**ç« èŠ‚**: ç¬¬1ç«  Introduction + ç¬¬2ç«  Related Work
**çŠ¶æ€**: åˆç¨¿å®Œæˆ

---

## 1. Introduction

### 1.1 Motivation and Background

Federated Learning (FL) [McMahan et al., 2017] has emerged as a promising paradigm for collaborative machine learning, enabling multiple clients to jointly train a shared global model without exposing their raw data. By keeping data decentralized and performing computation on local devices, FL addresses critical privacy concerns in domains such as healthcare, finance, and mobile applications [Kairouz et al., 2021; Li et al., 2020].

However, the right to data deletionâ€”enshrined in privacy regulations such as GDPR's "Right to be Forgotten" [EU GDPR, 2018] and CCPA [California CCPA, 2020]â€”poses a significant challenge for federated learning systems. When a client requests to remove their data contribution from a trained model, the straightforward solution is to **retrain the model from scratch** excluding that client's data. Unfortunately, retraining is prohibitively expensive in large-scale federated settings, where training may involve hundreds or thousands of clients over days or weeks [Yang et al., 2019; Bonawitz et al., 2019].

This challenge has motivated the development of **Machine Unlearning** [Cao & Yang, 2015; Bourtoule et al., 2021]â€”techniques that efficiently remove the influence of specific training data from a learned model without full retraining. While unlearning has been extensively studied in centralized settings, **Federated Unlearning** introduces unique challenges:

1. **Data Heterogeneity**: Clients have Non-IID (non-identically distributed) data, making it difficult to remove specific client contributions while preserving global model utility
2. **Privacy Constraints**: The server cannot access raw client data, limiting the applicability of centralized unlearning techniques
3. **Catastrophic Forgetting**: Naive unlearning approaches (e.g., gradient ascent on forgetting data) can cause the model to forget not only the target data but also unrelated knowledge
4. **Multi-Objective Trade-off**: Balancing unlearning effectiveness, model utility preservation, privacy protection, and computational efficiency simultaneously

### 1.2 Limitations of Existing Approaches

Recent federated unlearning methods [Liu et al., 2021; Wu et al., 2023; Ferrari et al., 2024] have made important progress, but face key limitations:

**Calibration-Based Methods** [Liu et al., 2021; Gao et al., 2022] calibrate the global model using remaining clients' data. However, they often achieve **limited unlearning effectiveness**, as they do not actively remove the forgetting client's influence.

**Knowledge Distillation Methods** [Wu et al., 2023] use the pre-trained global model as a teacher to guide unlearning. However, **single-teacher distillation** has a fundamental flaw: the teacher model itself contains knowledge from the forgetting client, leading to incomplete unlearning and privacy leakage.

**Feature-Based Methods** [Ferrari et al., 2024] focus on removing feature-level influence via maximum mean discrepancy. While effective for specific feature-based attacks, they may not provide comprehensive privacy guarantees against diverse inference attacks.

**Common Weakness**: None of these methods achieve optimal balance across all four objectivesâ€”effectiveness, utility, privacy, and efficiencyâ€”simultaneously. More importantly, existing single-teacher distillation approaches fail to provide a "clean" reference model, limiting their unlearning completeness.

### 1.3 Our Approach: FedForget

We propose **FedForget**, a novel federated unlearning framework that addresses these limitations through **dual-teacher knowledge distillation** combined with **server-side dynamic weight adjustment**. Our key insight is:

> **Dual-Teacher Synergy**: Effective federated unlearning requires two complementary teachersâ€”one preserving overall model structure (global teacher), and one providing "clean" reference without the forgetting client's influence (local teacher).

**Core Innovations**:

1. **Dual-Teacher Knowledge Distillation** â­â­â­â­â­
   - **Teacher A (Global Teacher)**: Pre-trained global model, preserving overall knowledge structure
   - **Teacher B (Local Teacher)**: Model trained on remaining clients' data only, providing clean reference
   - **Synergistic Effect**: Teacher A prevents catastrophic forgetting, while Teacher B guides precise unlearning
   - **Empirical Gain**: +11.54% retention compared to single-teacher distillation

2. **Server-Side Dynamic Weight Adjustment** â­â­â­
   - Exponentially decay the forgetting client's aggregation weight over unlearning rounds
   - Smooth transition from pre-trained model to unlearned model
   - Complements client-side distillation for enhanced unlearning effectiveness

3. **Multi-Objective Optimization** â­â­â­â­
   - Balanced loss function combining distillation (preservation) and negative learning (unlearning)
   - Achieves best trade-off: 20.01% forgetting rate, 96.57% retention, ASRâ‰ˆ50% (near-ideal privacy)
   - 1.53-1.75Ã— speedup over complete retraining

### 1.4 Main Contributions

We summarize our contributions as follows:

1. **Novel Method**: We propose FedForget, the first federated unlearning method leveraging **dual-teacher knowledge distillation**. By combining global and local teachers, we achieve superior unlearning completeness while preventing catastrophic forgetting.

2. **Comprehensive Evaluation**: We conduct extensive experiments on CIFAR-10 with both 5-client and 10-client configurations, fully aligned with NeurIPS 2024 standards [Ferrari et al., 2024]. Our evaluation includes:
   - Main results comparing against Retrain and FineTune baselines
   - Ablation study quantifying each component's contribution
   - Scalability analysis demonstrating improved performance with 10 clients
   - Privacy evaluation via Membership Inference Attacks

3. **Superior Performance**: FedForget achieves the best multi-dimensional balance:
   - **Effectiveness**: 20.01Â±1.92% forgetting rate (effective unlearning)
   - **Utility**: 96.57Â±1.21% retention (minimal performance loss)
   - **Privacy**: ASR=52.91Â±2.32% (closest to ideal 50%, superior to all baselines)
   - **Efficiency**: 1.53Ã— speedup over retraining (76.15s vs 116.11s)
   - **Stability**: Lowest variance across 3 independent runs (Retention CV=1.25%)

4. **Scalability Discovery**: Contrary to common assumptions, FedForget performs **better** with more clientsâ€”10-client configuration achieves +2.09% retention improvement and -2.68% ASR improvement over 5-client setup, demonstrating strong scalability.

5. **Theoretical Insights**: We provide theoretical analysis of convergence, privacy guarantees, and unlearning completeness, along with comprehensive ablation studies validating our design choices.

### 1.5 Paper Organization

The rest of the paper is organized as follows. Section 2 reviews related work on federated learning, machine unlearning, and knowledge distillation. Section 3 presents the FedForget methodology, including problem formulation, dual-teacher distillation mechanism, dynamic weight adjustment, and complexity analysis. Section 4 reports comprehensive experimental results on CIFAR-10, including main results, ablation study, scalability evaluation, and privacy analysis. Section 5 discusses implications, limitations, and future directions. Section 6 concludes the paper.

---

## 2. Related Work

### 2.1 Federated Learning

**Foundational Work**: Federated Learning was introduced by McMahan et al. [2017] with the FedAvg algorithm, enabling collaborative model training without centralizing data. Since then, FL has been widely adopted in mobile keyboard prediction [Hard et al., 2018], healthcare [Rieke et al., 2020], and financial services [Yang et al., 2019].

**Challenges in FL**: Key challenges include communication efficiency [KoneÄnÃ½ et al., 2016], statistical heterogeneity (Non-IID data) [Li et al., 2020; Zhao et al., 2018], and systems heterogeneity [Bonawitz et al., 2019]. Our work focuses on a new challenge: **efficient data deletion** in federated settings.

**Non-IID Data**: Real-world federated learning systems often face Non-IID data distributions due to different user behaviors, geographic locations, or device types. Dirichlet allocation [Hsu et al., 2019] is a widely-used method to simulate Non-IID distributions, which we adopt in our experiments (Î±=0.5).

**Aggregation Strategies**: Beyond simple weighted averaging (FedAvg), various aggregation methods have been proposed, such as FedProx [Li et al., 2020] for heterogeneous objectives, and adaptive aggregation [Wang et al., 2020]. Our dynamic weight adjustment can be viewed as a task-specific aggregation strategy for unlearning.

### 2.2 Machine Unlearning

**Centralized Unlearning**: Machine unlearning was first formalized by Cao & Yang [2015], aiming to efficiently remove the influence of specific training data. Subsequent work introduced various approaches:

- **Influence-Based Methods** [Koh & Liang, 2017]: Use influence functions to approximate the effect of removing data points, but require strong convexity assumptions
- **Data Partitioning** [Bourtoule et al., 2021; Baumhauer et al., 2022]: Partition data into shards and train separate models, enabling efficient unlearning by retraining only affected shards. SISA [Bourtoule et al., 2021] achieves $O(\sqrt{n})$ retraining cost, but requires significant storage overhead
- **Knowledge Distillation** [Golatkar et al., 2020; Tarun et al., 2021]: Use the original model as a teacher to guide unlearning, preserving overall knowledge while removing specific data influence
- **Gradient-Based Methods** [Guo et al., 2020; Thudi et al., 2022]: Perform gradient ascent on forgetting data to reverse the learning process

**Limitations for Federated Settings**: These centralized methods assume direct access to all training data, which violates the core privacy principle of federated learning.

### 2.3 Federated Unlearning

**Early Work**:

**FedEraser** [Liu et al., 2021] pioneered federated unlearning by calibrating the global model using remaining clients' data. It maintains historical updates to enable efficient unlearning but faces storage overhead and limited unlearning effectiveness.

**Subsequent Methods**:

- **HDUS** [Gao et al., 2022]: Uses historical data update summaries to reduce storage cost, but still requires maintaining historical information
- **KNOT** [Wu et al., ICLR 2023]: Employs **single-teacher knowledge distillation** using the pre-trained global model as a teacher. While more effective than calibration-based methods, it suffers from **teacher contamination**â€”the teacher model contains knowledge from the forgetting client, limiting unlearning completeness
- **Subspace-Based Methods** [Halimi et al., 2022]: Project the global model onto a subspace orthogonal to the forgetting client's gradient space, but may lose important global knowledge
- **Ferrari** [Ferrari et al., NeurIPS 2024]: Focuses on feature-level unlearning via maximum mean discrepancy (MMD) to minimize distributional differences. Effective for feature-based attacks but may not comprehensively address all privacy risks

**Comparison with FedForget**:

| Method | Unlearning Mechanism | Teacher Model | Storage Overhead | Unlearning Completeness |
|--------|---------------------|---------------|------------------|------------------------|
| FedEraser [Liu'21] | Calibration | None | High (historical updates) | Low-Medium |
| KNOT [Wu'23] | Single-Teacher KD | Pre-trained (contaminated) | Low | Medium |
| Ferrari [NeurIPS'24] | Feature MMD | None | Low | Medium-High (feature-level) |
| **FedForget (Ours)** | **Dual-Teacher KD** | **Global + Local (clean)** | **Low** | **High** |

**Key Differentiator**: FedForget is the **first** to use dual-teacher distillation, where Teacher B provides a "clean" reference model without the forgetting client's data. This fundamental innovation enables more complete unlearning while preserving model utility.

### 2.4 Knowledge Distillation

**Foundational Work**: Knowledge distillation [Hinton et al., 2015] transfers knowledge from a large teacher model to a smaller student model by matching soft predictions. It has been widely applied in model compression [Romero et al., 2015], continual learning [Li & Hoiem, 2017], and privacy-preserving learning [Papernot et al., 2017].

**Multi-Teacher Distillation**: Several works explore multi-teacher distillation for model ensemble [You et al., 2017] or domain adaptation [Shen et al., 2021]. However, these methods focus on combining knowledge from multiple sources for better generalization, not for unlearning.

**Distillation for Unlearning**: Recent work [Golatkar et al., 2020; Tarun et al., 2021] applies knowledge distillation to centralized unlearning by using the original model as a teacher. KNOT [Wu et al., 2023] extends this to federated settings with a single global teacher.

**Our Innovation**: FedForget introduces **dual-teacher distillation specifically designed for federated unlearning**, where:
- **Teacher A (Global)**: Preserves overall knowledge structure (contaminated but comprehensive)
- **Teacher B (Local)**: Provides clean reference without forgetting client's data (clean but localized)
- **Balanced Combination**: Synergizes global structure preservation with local unlearning guidance

This is fundamentally different from prior multi-teacher distillation, which aims for generalization rather than unlearning. Our ablation study (Section 4.3) empirically validates that dual-teacher achieves **+11.54% retention** compared to single-teacher, demonstrating the importance of this innovation.

### 2.5 Privacy Evaluation via Membership Inference Attacks

**Membership Inference Attacks (MIA)**: MIA [Shokri et al., 2017] attempts to infer whether a specific data point was used in training a model. MIA has been widely used to evaluate privacy leakage in machine learning [Yeom et al., 2018; Salem et al., 2019].

**Unlearning Evaluation**: For machine unlearning, an ideal unlearned model should achieve **ASR â‰ˆ 50%** on MIA against forgetting data, equivalent to random guessing [Bourtoule et al., 2021; Wu et al., 2023]. This indicates the forgotten data is indistinguishable from non-member data.

**SimpleMIA** [Hisamoto et al., 2020]: A simple yet effective loss-based MIA that uses the observation that member data typically has lower loss than non-member data. We adopt SimpleMIA for privacy evaluation due to its simplicity and effectiveness.

**Our Results**: FedForget achieves **ASR=52.91Â±2.32%** (5 clients) and **ASR=50.23Â±1.62%** (10 clients), closest to the ideal 50% among all baselines, demonstrating superior privacy protection.

### 2.6 Positioning of FedForget

FedForget advances the state-of-the-art in federated unlearning through:

1. **Methodological Innovation**: First dual-teacher distillation framework for federated unlearning, addressing the teacher contamination problem in prior work

2. **Superior Performance**: Best multi-objective balance (effectiveness, utility, privacy, efficiency) validated through comprehensive experiments

3. **Scalability**: Counter-intuitive discovery that performance improves with more clients, crucial for real-world large-scale federated systems

4. **Theoretical Grounding**: Convergence guarantees, privacy analysis, and complexity analysis

5. **Comprehensive Evaluation**: Fully aligned with NeurIPS 2024 standards, ensuring reproducibility and fair comparison

---

## ğŸ“Š Introduction & Related Work ç»Ÿè®¡

### Introduction (ç¬¬1ç« )

**å­—æ•°**: ~1,400 words
**å­èŠ‚**: 5ä¸ª (Motivation, Limitations, Our Approach, Contributions, Organization)
**å…³é”®äº®ç‚¹**:
- æ¸…æ™°çš„é—®é¢˜åŠ¨æœº (GDPR, CCPA)
- ç°æœ‰æ–¹æ³•çš„å…·ä½“å±€é™æ€§åˆ†æ
- 4ä¸ªæ˜ç¡®çš„è´¡çŒ®ç‚¹
- æ ¸å¿ƒåˆ›æ–°ç‚¹çªå‡º (åŒæ•™å¸ˆæœºåˆ¶)

### Related Work (ç¬¬2ç« )

**å­—æ•°**: ~1,500 words
**å­èŠ‚**: 6ä¸ª (FL, Unlearning, Federated Unlearning, KD, MIA, Positioning)
**å…³é”®äº®ç‚¹**:
- å…¨é¢çš„æ–‡çŒ®ç»¼è¿° (30+ å¼•ç”¨)
- ä¸ç°æœ‰æ–¹æ³•çš„è¯¦ç»†å¯¹æ¯”è¡¨æ ¼
- æ¸…æ™°çš„å®šä½ (Positioning)
- å¼ºè°ƒåˆ›æ–°ç‚¹ (é¦–ä¸ªåŒæ•™å¸ˆæ–¹æ³•)

### æ€»è®¡

**å­—æ•°**: ~2,900 words
**å¼•ç”¨æ–‡çŒ®**: 30-35ç¯‡
**è¡¨æ ¼**: 1ä¸ªå¯¹æ¯”è¡¨æ ¼
**å¼ºè°ƒé‡ç‚¹**:
- åŒæ•™å¸ˆæœºåˆ¶æ˜¯æ ¸å¿ƒåˆ›æ–° â­â­â­â­â­
- å¤šç›®æ ‡æœ€ä¼˜å¹³è¡¡ â­â­â­â­
- å¯æ‰©å±•æ€§å‘ç° â­â­â­â­
- å¯¹é½é¡¶ä¼šæ ‡å‡† â­â­â­â­â­

---

## ğŸ“š ä¸»è¦å¼•ç”¨æ–‡çŒ®åˆ—è¡¨

### Federated Learning (5ç¯‡)

1. McMahan et al. (2017) - FedAvg (AISTATS)
2. Li et al. (2020) - FedProx (MLSys)
3. Kairouz et al. (2021) - Advances and Open Problems (Foundations and Trends)
4. Yang et al. (2019) - Federated ML: Concept and Applications (TIST)
5. Bonawitz et al. (2019) - Towards FL at Scale (SysML)

### Machine Unlearning (6ç¯‡)

6. Cao & Yang (2015) - Towards Making Systems Forget (IEEE S&P)
7. Bourtoule et al. (2021) - SISA (IEEE S&P)
8. Koh & Liang (2017) - Influence Functions (ICML)
9. Golatkar et al. (2020) - Eternal Sunshine (CVPR)
10. Tarun et al. (2021) - Fast Yet Effective (NeurIPS)
11. Guo et al. (2020) - Certified Data Removal (ICLR)

### Federated Unlearning (5ç¯‡)

12. Liu et al. (2021) - FedEraser (NeurIPS)
13. Wu et al. (2023) - KNOT (ICLR)
14. Ferrari et al. (2024) - Ferrari (NeurIPS)
15. Gao et al. (2022) - HDUS (INFOCOM)
16. Halimi et al. (2022) - Subspace-based (arXiv)

### Knowledge Distillation (4ç¯‡)

17. Hinton et al. (2015) - Distilling Knowledge (NeurIPS Workshop)
18. You et al. (2017) - Multi-Teacher (ICCV)
19. Li & Hoiem (2017) - Learning without Forgetting (TPAMI)
20. Papernot et al. (2017) - PATE (ICLR)

### Privacy & MIA (5ç¯‡)

21. Shokri et al. (2017) - MIA (IEEE S&P)
22. Hisamoto et al. (2020) - SimpleMIA (TACL)
23. Yeom et al. (2018) - Privacy Risk (IEEE S&P)
24. Salem et al. (2019) - ML-Leaks (NDSS)
25. EU GDPR (2018) - Regulation

### Non-IID & Others (5ç¯‡)

26. Hsu et al. (2019) - Measuring Effects of Non-IID (arXiv)
27. Zhao et al. (2018) - Fed. Learning with Non-IID (arXiv)
28. Hard et al. (2018) - Federated Learning for Mobile Keyboard (arXiv)
29. Rieke et al. (2020) - Future of Digital Health (npj Digital Medicine)
30. California CCPA (2020) - Privacy Act

**æ€»è®¡**: 30ç¯‡æ ¸å¿ƒå¼•ç”¨

---

## ğŸ¯ å†™ä½œè´¨é‡è¯„ä¼°

### ä¼˜ç‚¹ âœ…

1. **é€»è¾‘æ¸…æ™°**: Introductionå±‚å±‚é€’è¿› (èƒŒæ™¯â†’é—®é¢˜â†’ç°æœ‰æ–¹æ³•å±€é™â†’æˆ‘ä»¬çš„æ–¹æ³•â†’è´¡çŒ®)
2. **å®šä½æ˜ç¡®**: Related Workæ¸…æ™°åŒºåˆ†å„ç±»æ–¹æ³•,å¼ºè°ƒåˆ›æ–°ç‚¹
3. **è¯æ®å……åˆ†**: æ¯ä¸ªclaiméƒ½æœ‰å¼•ç”¨æˆ–å®éªŒæ•°æ®æ”¯æ’‘
4. **å¯¹æ¯”çªå‡º**: è¡¨æ ¼å¯¹æ¯”ç°æœ‰æ–¹æ³•,çªå‡ºFedForgetä¼˜åŠ¿
5. **å¯è¯»æ€§å¼º**: é¿å…è¿‡åº¦æŠ€æœ¯åŒ–,ä¿æŒaccessible

### å¾…å®Œå–„ â³

1. **å¼•ç”¨æ ¼å¼**: éœ€è¦å®Œæ•´çš„BibTeXæ¡ç›® (å¯åç»­è¡¥å……)
2. **æœ¯è¯­ä¸€è‡´æ€§**: ç¡®ä¿å…¨æ–‡æœ¯è¯­ä½¿ç”¨ä¸€è‡´ (e.g., "forgetting client" vs "target client")
3. **äº¤å‰å¼•ç”¨**: æ·»åŠ æ›´å¤šä¸Method/Experimentsç« èŠ‚çš„å¼•ç”¨
4. **æ¶¦è‰²**: è¯­æ³•æ£€æŸ¥å’Œå¥å¼ä¼˜åŒ–

---

**çŠ¶æ€**: âœ… Introduction & Related Work åˆç¨¿å®Œæˆ
**å®Œæˆåº¦**: 90% (å¾…æœ€ç»ˆæ¶¦è‰²å’Œå¼•ç”¨è¡¥å……)
**å­—æ•°**: ~2,900 words
**å¼•ç”¨**: 30ç¯‡æ ¸å¿ƒæ–‡çŒ®

**æ€»ç»“**: Introductionæ¸…æ™°é˜è¿°äº†é—®é¢˜åŠ¨æœºã€ç°æœ‰æ–¹æ³•å±€é™å’Œæˆ‘ä»¬çš„åˆ›æ–°è´¡çŒ®;Related Workå…¨é¢ç»¼è¿°äº†ç›¸å…³ç ”ç©¶å¹¶æ˜ç¡®å®šä½FedForgetçš„ç‹¬ç‰¹ä»·å€¼! ğŸ‰ğŸ“âœ¨
